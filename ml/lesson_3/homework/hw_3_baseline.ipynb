{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:41:30.876278Z",
     "start_time": "2024-12-22T12:41:30.872072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "476f1de5d68fd337",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:41:31.449264Z",
     "start_time": "2024-12-22T12:41:31.445596Z"
    }
   },
   "cell_type": "code",
   "source": "RANDOM_STATE = 42",
   "id": "cc75e827cc849ff7",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T12:41:33.235554Z",
     "start_time": "2024-12-22T12:41:32.190096Z"
    }
   },
   "source": "df = pd.read_csv('./lenta_df.csv')",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:41:33.804630Z",
     "start_time": "2024-12-22T12:41:33.245555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "russian_stopwords = set(stopwords.words('russian'))"
   ],
   "id": "3e31b32ed1273fdc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gulfik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Gulfik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:42:12.735821Z",
     "start_time": "2024-12-22T12:41:34.831137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word.lower() not in russian_stopwords]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "df['text'] = df['text'].dropna().apply(preprocess_text)\n",
    "cleaned_df = df.copy()\n",
    "cleaned_df.shape"
   ],
   "id": "f382f3962ec57584",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:42:12.748997Z",
     "start_time": "2024-12-22T12:42:12.740820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_df['text'], cleaned_df['bloc'], test_size=0.2,\n",
    "                                                    random_state=RANDOM_STATE)"
   ],
   "id": "70d7b11e10138c3f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:42:29.844839Z",
     "start_time": "2024-12-22T12:42:12.771894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ],
   "id": "e3cce727d3e2436e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:42:33.848390Z",
     "start_time": "2024-12-22T12:42:29.866358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_lg = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=RANDOM_STATE)\n",
    "model_lg.fit(X_train_tfidf, y_train)\n",
    "model_lg_pred = model_lg.predict(X_test_tfidf)"
   ],
   "id": "57ef92dd6d2eb079",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:42:33.905550Z",
     "start_time": "2024-12-22T12:42:33.870067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_tfidf, y_train)\n",
    "model_nb_pred = model_nb.predict(X_test_tfidf)"
   ],
   "id": "bcfa834bbfd4993e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:42:51.790318Z",
     "start_time": "2024-12-22T12:42:33.929410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=200, max_depth=15)\n",
    "model_rf.fit(X_train_tfidf, y_train)\n",
    "model_rf_pred = model_rf.predict(X_test_tfidf)"
   ],
   "id": "2b9d6c6c8c09e56d",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:43:55.709663Z",
     "start_time": "2024-12-22T12:43:50.983031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model_svm = LinearSVC(random_state=RANDOM_STATE, C=1)\n",
    "model_svm.fit(X_train_tfidf, y_train)\n",
    "model_svm_pred = model_svm.predict(X_test_tfidf)"
   ],
   "id": "722250ada9971fbf",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:43:59.943938Z",
     "start_time": "2024-12-22T12:43:55.713662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_tree = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=10)\n",
    "model_tree.fit(X_train_tfidf, y_train)\n",
    "model_tree_pred = model_tree.predict(X_test_tfidf)"
   ],
   "id": "6ef0cfc29a44fdd3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:54:58.513171Z",
     "start_time": "2024-12-22T12:43:59.971564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_xgb = XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='mlogloss', n_estimators=200,\n",
    "                      max_depth=10)\n",
    "model_xgb.fit(X_train_tfidf, y_train)\n",
    "model_xgb_pred = model_xgb.predict(X_test_tfidf)"
   ],
   "id": "1787cbdb2e5aadcd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:44:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T12:54:58.647716Z",
     "start_time": "2024-12-22T12:54:58.584799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Logistic regression report: {classification_report(y_test, model_lg_pred)}')\n",
    "print(f'NB report: {classification_report(y_test, model_nb_pred)}')\n",
    "print(f'Random forest report: {classification_report(y_test, model_rf_pred)}')\n",
    "print(f'SVM report: {classification_report(y_test, model_svm_pred)}')\n",
    "print(f'Decision tree report: {classification_report(y_test, model_tree_pred)}')\n",
    "print(f'XGB classifier report: {classification_report(y_test, model_xgb_pred)}')"
   ],
   "id": "adcaa7c5cc4777e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84       803\n",
      "           1       0.83      0.88      0.85       763\n",
      "           2       0.93      0.95      0.94       774\n",
      "           3       0.91      0.93      0.92       834\n",
      "           4       0.99      0.99      0.99       794\n",
      "           5       0.99      1.00      0.99       813\n",
      "           6       0.86      0.77      0.81       806\n",
      "           7       0.98      0.98      0.98       805\n",
      "           8       0.94      0.95      0.95       808\n",
      "\n",
      "    accuracy                           0.92      7200\n",
      "   macro avg       0.92      0.92      0.92      7200\n",
      "weighted avg       0.92      0.92      0.92      7200\n",
      "\n",
      "NB report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       803\n",
      "           1       0.74      0.79      0.76       763\n",
      "           2       0.81      0.96      0.88       774\n",
      "           3       0.82      0.93      0.87       834\n",
      "           4       0.99      0.94      0.96       794\n",
      "           5       0.92      1.00      0.96       813\n",
      "           6       0.67      0.50      0.57       806\n",
      "           7       0.94      0.94      0.94       805\n",
      "           8       0.93      0.93      0.93       808\n",
      "\n",
      "    accuracy                           0.85      7200\n",
      "   macro avg       0.85      0.85      0.85      7200\n",
      "weighted avg       0.85      0.85      0.85      7200\n",
      "\n",
      "Random forest report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.53      0.65       803\n",
      "           1       0.61      0.82      0.70       763\n",
      "           2       0.81      0.91      0.86       774\n",
      "           3       0.80      0.86      0.83       834\n",
      "           4       0.95      0.98      0.96       794\n",
      "           5       0.93      0.99      0.96       813\n",
      "           6       0.90      0.75      0.82       806\n",
      "           7       0.94      0.94      0.94       805\n",
      "           8       0.94      0.89      0.91       808\n",
      "\n",
      "    accuracy                           0.85      7200\n",
      "   macro avg       0.86      0.85      0.85      7200\n",
      "weighted avg       0.86      0.85      0.85      7200\n",
      "\n",
      "SVM report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       803\n",
      "           1       0.86      0.88      0.87       763\n",
      "           2       0.94      0.95      0.94       774\n",
      "           3       0.93      0.93      0.93       834\n",
      "           4       0.99      1.00      0.99       794\n",
      "           5       0.99      1.00      0.99       813\n",
      "           6       0.86      0.83      0.85       806\n",
      "           7       0.98      0.98      0.98       805\n",
      "           8       0.96      0.95      0.95       808\n",
      "\n",
      "    accuracy                           0.93      7200\n",
      "   macro avg       0.93      0.93      0.93      7200\n",
      "weighted avg       0.93      0.93      0.93      7200\n",
      "\n",
      "Decision tree report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.14      0.24       803\n",
      "           1       0.18      0.92      0.30       763\n",
      "           2       0.91      0.40      0.55       774\n",
      "           3       0.68      0.58      0.63       834\n",
      "           4       1.00      0.81      0.90       794\n",
      "           5       0.97      0.50      0.66       813\n",
      "           6       0.90      0.72      0.80       806\n",
      "           7       0.96      0.26      0.41       805\n",
      "           8       0.97      0.23      0.37       808\n",
      "\n",
      "    accuracy                           0.51      7200\n",
      "   macro avg       0.81      0.51      0.54      7200\n",
      "weighted avg       0.81      0.51      0.54      7200\n",
      "\n",
      "XGB classifier report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       803\n",
      "           1       0.88      0.89      0.88       763\n",
      "           2       0.95      0.93      0.94       774\n",
      "           3       0.91      0.93      0.92       834\n",
      "           4       1.00      0.99      0.99       794\n",
      "           5       0.99      0.99      0.99       813\n",
      "           6       0.89      0.90      0.90       806\n",
      "           7       0.97      0.96      0.97       805\n",
      "           8       0.95      0.93      0.94       808\n",
      "\n",
      "    accuracy                           0.93      7200\n",
      "   macro avg       0.93      0.93      0.93      7200\n",
      "weighted avg       0.93      0.93      0.93      7200\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:26:32.023393Z",
     "start_time": "2024-12-22T13:24:10.799722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svm', LinearSVC(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # 'tfidf__max_features': [5000, 10000, 20000],\n",
    "    # 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    # 'tfidf__min_df': [1, 2, 5],\n",
    "    # 'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "    'svm__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'svm__loss': ['hinge', 'squared_hinge'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая точность на кросс-валидации:\", grid_search.best_score_)\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "ec3ec98cbb745ad0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "15 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 317, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "                                           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1214, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1046, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.86121528 0.86354167        nan 0.86354167 0.88850694 0.91590278\n",
      "        nan 0.91590278 0.93381944 0.92989583        nan 0.92996528\n",
      " 0.92993056 0.92940972        nan 0.92944444 0.92996528 0.92888889\n",
      "        nan 0.9296875 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'svm__C': 1, 'svm__dual': True, 'svm__loss': 'hinge'}\n",
      "Лучшая точность на кросс-валидации: 0.9338194444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       803\n",
      "           1       0.87      0.88      0.88       763\n",
      "           2       0.94      0.95      0.95       774\n",
      "           3       0.92      0.94      0.93       834\n",
      "           4       0.99      1.00      1.00       794\n",
      "           5       0.99      1.00      0.99       813\n",
      "           6       0.87      0.84      0.85       806\n",
      "           7       0.98      0.99      0.98       805\n",
      "           8       0.95      0.96      0.96       808\n",
      "\n",
      "    accuracy                           0.94      7200\n",
      "   macro avg       0.93      0.93      0.93      7200\n",
      "weighted avg       0.93      0.94      0.93      7200\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:33:58.617422Z",
     "start_time": "2024-12-22T13:27:15.271137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # 'tfidf__max_features': [5000, 10000, 20000],\n",
    "    # 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    # 'tfidf__min_df': [1, 2, 5],\n",
    "    # 'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "    'clf__C': [0.1, 1, 10, 100],\n",
    "    'clf__solver': ['lbfgs', 'liblinear'],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__max_iter': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "\n",
    "best_lg_model = grid_search.best_estimator_\n",
    "best_lg_pred = best_lg_model.predict(X_test)\n",
    "print(classification_report(y_test, best_lg_pred))"
   ],
   "id": "dbf178d8221aad03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Лучшие параметры: {'clf__C': 100, 'clf__max_iter': 500, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       803\n",
      "           1       0.86      0.88      0.87       763\n",
      "           2       0.93      0.96      0.95       774\n",
      "           3       0.92      0.94      0.93       834\n",
      "           4       0.99      0.99      0.99       794\n",
      "           5       0.99      1.00      0.99       813\n",
      "           6       0.86      0.80      0.83       806\n",
      "           7       0.98      0.99      0.98       805\n",
      "           8       0.96      0.96      0.96       808\n",
      "\n",
      "    accuracy                           0.93      7200\n",
      "   macro avg       0.93      0.93      0.93      7200\n",
      "weighted avg       0.93      0.93      0.93      7200\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:12:31.511734Z",
     "start_time": "2024-12-21T21:12:31.507442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Shape of X_train_tfidf:\", X_train_tfidf.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ],
   "id": "730d989878923c9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_tfidf: (25600, 5000)\n",
      "Length of y_train: 25600\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:53:20.874256Z",
     "start_time": "2024-12-21T20:51:09.333148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "c89ae5472c054659",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gulfik\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:51:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       785\n",
      "           1       0.94      0.94      0.94       819\n",
      "           2       0.97      0.96      0.96       761\n",
      "           3       0.93      0.95      0.94       810\n",
      "           4       0.99      0.98      0.99       782\n",
      "           5       0.98      0.99      0.99       785\n",
      "           6       0.99      0.97      0.98       798\n",
      "           7       0.97      0.96      0.97       860\n",
      "\n",
      "    accuracy                           0.95      6400\n",
      "   macro avg       0.96      0.95      0.96      6400\n",
      "weighted avg       0.96      0.95      0.95      6400\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:12:33.410631Z",
     "start_time": "2024-12-21T21:12:31.533555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "id": "1d7240688ee5981e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:41:22.182926Z",
     "start_time": "2024-12-22T13:40:37.992203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('./test_news.csv')\n",
    "test_df['cleaned_text'] = test_df['content'].apply(preprocess_text)\n",
    "y_pred_test = best_pipeline.predict(test_df['cleaned_text'])\n",
    "pd.DataFrame({'topic': y_pred_test}).reset_index().to_csv('submission.csv', index=False)"
   ],
   "id": "213416893a06bfbb",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T13:37:06.589724Z",
     "start_time": "2024-12-22T13:36:17.637015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('./test_news.csv')\n",
    "test_df['cleaned_text'] = test_df['content'].apply(preprocess_text)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(test_df['cleaned_text'])\n",
    "y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "pd.DataFrame({'topic': y_pred_test}).reset_index().to_csv('submission.csv', index=False)"
   ],
   "id": "305592b986b1da94",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcleaned_text\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(preprocess_text)\n\u001B[0;32m      4\u001B[0m X_test_tfidf \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mtransform(test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcleaned_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 5\u001B[0m y_pred_test \u001B[38;5;241m=\u001B[39m \u001B[43mbest_lg_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_tfidf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtopic\u001B[39m\u001B[38;5;124m'\u001B[39m: y_pred_test})\u001B[38;5;241m.\u001B[39mreset_index()\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubmission.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:600\u001B[0m, in \u001B[0;36mPipeline.predict\u001B[1;34m(self, X, **params)\u001B[0m\n\u001B[0;32m    598\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _routing_enabled():\n\u001B[0;32m    599\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(with_final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 600\u001B[0m         Xt \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mpredict(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    603\u001B[0m \u001B[38;5;66;03m# metadata routing enabled\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2115\u001B[0m, in \u001B[0;36mTfidfVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   2098\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001B[39;00m\n\u001B[0;32m   2099\u001B[0m \n\u001B[0;32m   2100\u001B[0m \u001B[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2111\u001B[0m \u001B[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001B[39;00m\n\u001B[0;32m   2112\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2113\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m, msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe TF-IDF vectorizer is not fitted\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2115\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mtransform(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1417\u001B[0m, in \u001B[0;36mCountVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   1414\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_vocabulary()\n\u001B[0;32m   1416\u001B[0m \u001B[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001B[39;00m\n\u001B[1;32m-> 1417\u001B[0m _, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1419\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1257\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1258\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1259\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1260\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1261\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32m~\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:108\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m preprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 108\u001B[0m         doc \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    110\u001B[0m         doc \u001B[38;5;241m=\u001B[39m tokenizer(doc)\n",
      "File \u001B[1;32m~\\Desktop\\petprojects\\hseContest\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:66\u001B[0m, in \u001B[0;36m_preprocess\u001B[1;34m(doc, accent_function, lower)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;124;03mapply to a document.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;124;03m    preprocessed string\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lower:\n\u001B[1;32m---> 66\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m()\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accent_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     68\u001B[0m     doc \u001B[38;5;241m=\u001B[39m accent_function(doc)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'csr_matrix' object has no attribute 'lower'"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T20:50:11.277163900Z",
     "start_time": "2024-12-21T20:30:13.745955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('./test_news.csv')\n",
    "test_df['cleaned_text'] = test_df['content'].apply(preprocess_text)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(test_df['cleaned_text'])\n",
    "y_pred_test = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "pd.DataFrame({'topic': y_pred_test}).reset_index().to_csv('submission.csv', index=False)"
   ],
   "id": "81819b3d3bd1c988",
   "outputs": [],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
